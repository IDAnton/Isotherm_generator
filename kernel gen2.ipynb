{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __int__(self):\n",
    "\n",
    "\n",
    "   def backprop(self, x, y):\n",
    "        \"\"\"Вернуть кортеж ``(nabla_b, nabla_w)``, представляющий градиент для функции стоимости C_x.  ``nabla_b`` и ``nabla_w`` - послойные списки массивов numpy, похожие на ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # прямой проход\n",
    "        activation = x\n",
    "        activations = [x] # список для послойного хранения активаций\n",
    "        zs = [] # список для послойного хранения z-векторов\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # обратный проход\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \"\"\"Переменная l в цикле ниже используется не так, как описано во второй главе книги. l = 1 означает последний слой нейронов, l = 2 – предпоследний, и так далее. Мы пользуемся преимуществом того, что в python можно использовать отрицательные индексы в массивах.\"\"\"\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "...\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Вернуть вектор частных производных (чп C_x / чп a) для выходных активаций.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Сигмоида.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Производная сигмоиды.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test):\n",
    "    for epoch in range (1, 200):\n",
    "        loss = 0\n",
    "        inner_counter = 1\n",
    "        for i in np.random.permutation(len(X_train)):\n",
    "            prediction = model.run(data=X_train[i].flatten(), activation_function=relu)\n",
    "            loss += np.square(y_train[i] - prediction[0][0])/2\n",
    "            model.calculate_gradient(data=X_train[i].reshape(2, 1), right_out=y_train[i].flatten(), activation_deriv=relu)\n",
    "            #if inner_counter % 50 == 0:\n",
    "            model.apply_gradient(batch_size=1)\n",
    "            #inner_counter += 1\n",
    "\n",
    "        test_loss = 0\n",
    "        # for i in range(0, 10000):\n",
    "        #     prediction = np.argmax(model.run(data=X_test[i], activation_function=relu))\n",
    "        #     test_loss += (y_test[i] - prediction) ** 2\n",
    "        print(f\"Epoch №{epoch} finished with accuracy {round(loss/len(X_train), 4)}% Test dataset accuracy {round(test_loss/len(X_train) * 100, 2)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "P_START = 21\n",
    "dataframe_sorb = pd.read_excel('Silica-loc-isoth1.xlsx', header=None, sheet_name=\"Adsorption\")\n",
    "AX_train = []\n",
    "PX_train = []\n",
    "pore_N = dataframe_sorb.shape[1]\n",
    "AX_train = np.array(dataframe_sorb.iloc[0][1:])\n",
    "PX_train = np.array(dataframe_sorb[0][P_START:])\n",
    "ax_scale = np.max(AX_train)\n",
    "px_scale = np.max(PX_train)\n",
    "AX_train = AX_train / ax_scale\n",
    "PX_train = PX_train / px_scale\n",
    "\n",
    "X_train = np.empty(shape = (len(AX_train) * len(PX_train), 2))\n",
    "Y_train = np.empty(shape =len(AX_train) * len(PX_train))\n",
    "k = 0\n",
    "for i in range(0, len(AX_train)):\n",
    "    for j in range(0, len(PX_train)):\n",
    "        X_train[k] = np.array([AX_train[i], PX_train[j]])\n",
    "        Y_train[k] = dataframe_sorb[i+1][j+P_START]\n",
    "        k+=1\n",
    "y_scale = np.max(Y_train)\n",
    "Y_train = Y_train / y_scale"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "model = Perceptron(input_dim=2, h1_dim=100, out_dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch №1 finished with accuracy 0.0016% Test dataset accuracy 0.0\n",
      "Epoch №2 finished with accuracy 0.0016% Test dataset accuracy 0.0\n",
      "Epoch №3 finished with accuracy 0.0016% Test dataset accuracy 0.0\n",
      "Epoch №4 finished with accuracy 0.0016% Test dataset accuracy 0.0\n",
      "Epoch №5 finished with accuracy 0.0016% Test dataset accuracy 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m model\u001B[38;5;241m.\u001B[39mlearn_rate \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.05\u001B[39m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[11], line 10\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, X_train, y_train, X_test, y_test)\u001B[0m\n\u001B[0;32m      8\u001B[0m     model\u001B[38;5;241m.\u001B[39mcalculate_gradient(data\u001B[38;5;241m=\u001B[39mX_train[i]\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m), right_out\u001B[38;5;241m=\u001B[39my_train[i]\u001B[38;5;241m.\u001B[39mflatten(), activation_deriv\u001B[38;5;241m=\u001B[39mrelu)\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;66;03m#if inner_counter % 50 == 0:\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_gradient\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;66;03m#inner_counter += 1\u001B[39;00m\n\u001B[0;32m     13\u001B[0m test_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "Cell \u001B[1;32mIn[5], line 74\u001B[0m, in \u001B[0;36mPerceptron.apply_gradient\u001B[1;34m(self, batch_size)\u001B[0m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb1 \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearn_rate \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdE_db1 \u001B[38;5;241m/\u001B[39m batch_size\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb2 \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearn_rate \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdE_db2 \u001B[38;5;241m/\u001B[39m batch_size\n\u001B[1;32m---> 74\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop_gradients\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[5], line 43\u001B[0m, in \u001B[0;36mPerceptron.drop_gradients\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdrop_gradients\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m---> 43\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdE_dw1 \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mh1_dim\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     44\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdE_db1 \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros((\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mh1_dim))\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdE_dw2 \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mh1_dim, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_dim))\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model.learn_rate = 0.05\n",
    "train_model(model, X_train, Y_train, X_train, Y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0094732]] 0.00646839989349246\n"
     ]
    }
   ],
   "source": [
    "i = 8000\n",
    "a = []\n",
    "right_answer = model.run(data=X_train[i].flatten(), activation_function=relu)\n",
    "#dataframe_sorb = pd.read_excel('Silica-loc-isoth1.xlsx', header=None, sheet_name=\"Adsorption\")\n",
    "# data_sorb = dataframe_sorb.to_numpy()\n",
    "# pressures_d = data_sorb[:, 0][21:]\n",
    "# plt.plot(pressures_d, y_train[i] * y_scale, marker=\".\")\n",
    "# plt.plot(pressures_d, right_answer.reshape(458, 1) * y_scale , marker=\".\")\n",
    "print(right_answer*y_scale, Y_train[i]*y_scale)\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}